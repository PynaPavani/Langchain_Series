{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c1e4a4e",
   "metadata": {},
   "source": [
    "# Using only with GOOGLE PALM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13b7b091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dfb1cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "google_api_key=os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "588fd104",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_api_key='AIzaSyAKkEKkTcn8uznz3GrStUppv18waZ4gEEE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ba5cccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as palm\n",
    "palm.configure(api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c295643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter you query: hi\n"
     ]
    }
   ],
   "source": [
    "user_query=input('enter you query: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31514737",
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"Explain about the {topic} within 2 line\"\n",
    "prompt_template=PromptTemplate(\n",
    "                    template=template,\n",
    "                    input_variables=['topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f79f1fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=prompt_template.format(topic=user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e59c8012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Hi** is a friendly greeting. It can be used to greet someone you know or someone you don't know.\n"
     ]
    }
   ],
   "source": [
    "completion= palm.generate_text(model='models/text-bison-001',\n",
    "                              prompt=prompt,temperature=0.2)\n",
    "print(completion.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b17bad",
   "metadata": {},
   "source": [
    "# Using langchain with palm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c90beb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **Sai**\n",
      "2. **Krishna**\n",
      "3. **Sai Ram**\n",
      "4. **Sai Narayan**\n",
      "5. **Sai Vishnu**\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as palm\n",
    "\n",
    "\n",
    "google_api_key='AIzaSyAKkEKkTcn8uznz3GrStUppv18waZ4gEEE'\n",
    "palm.configure(api_key=google_api_key)\n",
    "\n",
    "prompt='give me 5 other suitable names for my name saikrishna'\n",
    "\n",
    "completion = palm.generate_text(\n",
    "    model='models/text-bison-001',\n",
    "    prompt=prompt,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(completion.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98f99a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'give me 5 suitable names for my name sai'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "template_name=PromptTemplate(template=\"give me 5 suitable names for my name {name}\",input_variables=['name'])\n",
    "template_name.format(name='sai')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c80586b",
   "metadata": {},
   "source": [
    "# Using PalmAPI2 With langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a93b8a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getenv('GOOGLE_API_KEY')\n",
    "from dotenv import load_dotenv\n",
    "google_api_key='AIzaSyAKkEKkTcn8uznz3GrStUppv18waZ4gEEE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81a54b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as palm\n",
    "palm.configure(api_key=google_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8902d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/text-bison-001\n"
     ]
    }
   ],
   "source": [
    "models = [m for m in palm.list_models() if 'generateText' in m.supported_generation_methods]\n",
    "model = models[0].name\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85f3078a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sky is not pink.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt=\" why sky is pink?\"\n",
    "name=palm.generate_text(prompt=prompt, model=model, temperature=0.3,max_output_tokens=60)\n",
    "name.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e285b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with lang chain\n",
    "\n",
    "from langchain.embeddings import GooglePalmEmbeddings\n",
    "from langchain.llms import GooglePalm\n",
    "\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "llm= GooglePalm()\n",
    "llm.temperature=0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d10b3f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hot'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [\"The opposite of hot is\",'The opposite of cold is']\n",
    "llm_result = llm._generate(prompts)\n",
    "\n",
    "llm_result.generations[0][0].text\n",
    "llm_result.generations[1][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f96e85c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwho you are?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m template\u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39m_generate(prompt)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerations\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt=\"who you are?\"\n",
    "template= llm._generate(prompt)\n",
    "template.generations[0][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d889413",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected EmbeddingFunction.__call__ to have the following signature: odict_keys(['self', 'input']), got odict_keys(['self', 'args', 'kwargs'])\nPlease see https://docs.trychroma.com/embeddings for details of the EmbeddingFunction interface.\nPlease note the recent change to the EmbeddingFunction interface: https://docs.trychroma.com/migration#migration-to-0416---november-7-2023 \n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m urls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://en.wikipedia.org/wiki/Taj_Mahal/\u001b[39m\u001b[38;5;124m'\u001b[39m,]\n\u001b[1;32m     15\u001b[0m loader \u001b[38;5;241m=\u001b[39m [UnstructuredURLLoader(urls\u001b[38;5;241m=\u001b[39murls)]\n\u001b[0;32m---> 16\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mVectorstoreIndexCreator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGooglePalmEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_splitter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCharacterTextSplitter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_overlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_loaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m chain \u001b[38;5;241m=\u001b[39m RetrievalQA\u001b[38;5;241m.\u001b[39mfrom_chain_type(llm\u001b[38;5;241m=\u001b[39mllm, \n\u001b[1;32m     23\u001b[0m                             chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     24\u001b[0m                             retriever\u001b[38;5;241m=\u001b[39mindex\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39mas_retriever(), \n\u001b[1;32m     25\u001b[0m                             input_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m answer \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhere is TajMahal is Located?\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/langchain/indexes/vectorstore.py:82\u001b[0m, in \u001b[0;36mVectorstoreIndexCreator.from_loaders\u001b[0;34m(self, loaders)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m loader \u001b[38;5;129;01min\u001b[39;00m loaders:\n\u001b[1;32m     81\u001b[0m     docs\u001b[38;5;241m.\u001b[39mextend(loader\u001b[38;5;241m.\u001b[39mload())\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/langchain/indexes/vectorstore.py:87\u001b[0m, in \u001b[0;36mVectorstoreIndexCreator.from_documents\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a vectorstore index from documents.\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m sub_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_splitter\u001b[38;5;241m.\u001b[39msplit_documents(documents)\n\u001b[0;32m---> 87\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorstore_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43msub_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorstore_kwargs\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m VectorStoreIndexWrapper(vectorstore\u001b[38;5;241m=\u001b[39mvectorstore)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/langchain/vectorstores/chroma.py:612\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    611\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/langchain/vectorstores/chroma.py:567\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[Chroma],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    547\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Chroma:\n\u001b[1;32m    548\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \n\u001b[1;32m    550\u001b[0m \u001b[38;5;124;03m    If a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03m        Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m     chroma_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m     chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(texts\u001b[38;5;241m=\u001b[39mtexts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mids)\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chroma_collection\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/langchain/vectorstores/chroma.py:125\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[0;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persist_directory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    121\u001b[0m         _client_settings\u001b[38;5;241m.\u001b[39mpersist_directory \u001b[38;5;129;01mor\u001b[39;00m persist_directory\n\u001b[1;32m    122\u001b[0m     )\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;241m=\u001b[39m embedding_function\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_or_create_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverride_relevance_score_fn \u001b[38;5;241m=\u001b[39m relevance_score_fn\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/chromadb/api/client.py:226\u001b[0m, in \u001b[0;36mClient.get_or_create_collection\u001b[0;34m(self, name, metadata, embedding_function, data_loader)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_or_create_collection\u001b[39m(\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m     data_loader: Optional[DataLoader[Loadable]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    225\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Collection:\n\u001b[0;32m--> 226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_or_create_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/chromadb/telemetry/opentelemetry/__init__.py:127\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/chromadb/api/segment.py:216\u001b[0m, in \u001b[0;36mSegmentAPI.get_or_create_collection\u001b[0;34m(self, name, metadata, embedding_function, data_loader, tenant, database)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;129m@trace_method\u001b[39m(\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSegmentAPI.get_or_create_collection\u001b[39m\u001b[38;5;124m\"\u001b[39m, OpenTelemetryGranularity\u001b[38;5;241m.\u001b[39mOPERATION\n\u001b[1;32m    203\u001b[0m )\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     database: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_DATABASE,\n\u001b[1;32m    215\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Collection:\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_or_create\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/chromadb/telemetry/opentelemetry/__init__.py:127\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/chromadb/api/segment.py:190\u001b[0m, in \u001b[0;36mSegmentAPI.create_collection\u001b[0;34m(self, name, metadata, embedding_function, data_loader, get_or_create, tenant, database)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_product_telemetry_client\u001b[38;5;241m.\u001b[39mcapture(\n\u001b[1;32m    183\u001b[0m     ClientCreateCollectionEvent(\n\u001b[1;32m    184\u001b[0m         collection_uuid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m),\n\u001b[1;32m    185\u001b[0m         embedding_function\u001b[38;5;241m=\u001b[39membedding_function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    186\u001b[0m     )\n\u001b[1;32m    187\u001b[0m )\n\u001b[1;32m    188\u001b[0m add_attributes_to_current_span({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollection_uuid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m)})\n\u001b[0;32m--> 190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCollection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoll\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoll\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/chromadb/api/models/Collection.py:85\u001b[0m, in \u001b[0;36mCollection.__init__\u001b[0;34m(self, client, name, id, embedding_function, data_loader, tenant, database, metadata)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Check to make sure the embedding function has the right signature, as defined by the EmbeddingFunction protocol\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[43mvalidate_embedding_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;241m=\u001b[39m embedding_function\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_loader \u001b[38;5;241m=\u001b[39m data_loader\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/chromadb/api/types.py:210\u001b[0m, in \u001b[0;36mvalidate_embedding_function\u001b[0;34m(embedding_function)\u001b[0m\n\u001b[1;32m    207\u001b[0m protocol_signature \u001b[38;5;241m=\u001b[39m signature(EmbeddingFunction\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m function_signature \u001b[38;5;241m==\u001b[39m protocol_signature:\n\u001b[0;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    211\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected EmbeddingFunction.__call__ to have the following signature: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprotocol_signature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction_signature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see https://docs.trychroma.com/embeddings for details of the EmbeddingFunction interface.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease note the recent change to the EmbeddingFunction interface: https://docs.trychroma.com/migration#migration-to-0416---november-7-2023 \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    214\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected EmbeddingFunction.__call__ to have the following signature: odict_keys(['self', 'input']), got odict_keys(['self', 'args', 'kwargs'])\nPlease see https://docs.trychroma.com/embeddings for details of the EmbeddingFunction interface.\nPlease note the recent change to the EmbeddingFunction interface: https://docs.trychroma.com/migration#migration-to-0416---november-7-2023 \n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader  #load urls into docoument-loader\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from sqlalchemy.engine import URL\n",
    "from langchain.indexes import VectorstoreIndexCreator #vectorize db index with chromadb\n",
    "from langchain.text_splitter import CharacterTextSplitter #text splitter\n",
    "\n",
    "from langchain.embeddings import GooglePalmEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain.document_loaders import UnstructuredPDFLoader  #load pdf\n",
    "\n",
    "\n",
    "\n",
    "urls = ['https://en.wikipedia.org/wiki/Taj_Mahal/',]\n",
    "loader = [UnstructuredURLLoader(urls=urls)]\n",
    "index = VectorstoreIndexCreator(\n",
    "        embedding=GooglePalmEmbeddings(),\n",
    "        text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)).from_loaders(loader)\n",
    "\n",
    "\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, \n",
    "                            chain_type=\"stuff\", \n",
    "                            retriever=index.vectorstore.as_retriever(), \n",
    "                            input_key=\"question\")\n",
    "\n",
    "\n",
    "answer = chain.run('Where is TajMahal is Located?')\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94e6db99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql+psycopg2://user:***@localhost:5432/mydatabase\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy.engine import URL\n",
    "\n",
    "url = URL.create(\n",
    "    drivername=\"postgresql+psycopg2\",\n",
    "    username=\"user\",\n",
    "    password=\"password\",\n",
    "    host=\"localhost\",\n",
    "    port=5432,\n",
    "    database=\"mydatabase\"\n",
    ")\n",
    "print(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379c9ff5",
   "metadata": {},
   "source": [
    "# using prompt template , and chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "750d32d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A large language model (LLM) is a type of artificial intelligence (AI) program that can recognize and generate text, among other tasks. LLMs are trained on huge sets of data — hence the name \"large.\" LLMs are built on machine learning: specifically, a type of neural network called a transformer model.\n",
      "\n",
      "LLMs are able to perform a variety of tasks, including:\n",
      "\n",
      "* Recognizing and generating text\n",
      "* Translating languages\n",
      "* Summarizing text\n",
      "* Answering questions\n",
      "* Writing different kinds of creative content, such as poems, code, scripts, etc.\n",
      "\n",
      "LLMs are still under development, but they have the potential to revolutionize the way we interact with computers. In the future, LLMs could be used to help us with tasks such as writing emails, translating documents, and searching for information. They could also be used to create new forms of art and entertainment.\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    " large language model (LLM) is a type of artificial intelligence (AI) program that can recognize and generate text, among other tasks. LLMs are trained on huge sets of data — hence the name \"large.\" LLMs are built on machine learning: specifically, a type of neural network called a transformer model.\n",
    "\n",
    "{question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=False)\n",
    "\n",
    "print(llm_chain.run('What do you understand with this para?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fdd96fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate,LLMChain\n",
    "\n",
    "template=\"i have restaurant of {cusine} food. suggesst me 1 name.\"\n",
    "prompt_name=PromptTemplate(llm=llm,template=template,input_variables=['cusine'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "979460e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res= LLMChain(llm=llm,prompt=prompt_name,verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24540ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mi have restaurant of american food. suggesst me 1 name.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'**The American Dream**'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.run('american')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4647f359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. **The American Dream Burger**\\n2. **Mac and Cheese**\\n3. **Fried Chicken**\\n4. **Fries**\\n5. **Milkshake**\\n6. **Apple Pie**'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate,LLMChain\n",
    "\n",
    "template=\"i have restaurant of {cusine} food. suggest me 1 name.\"\n",
    "prompt_name=PromptTemplate(llm=llm,template=template,input_variables=['cusine'])\n",
    "res_name= LLMChain(llm=llm,prompt=prompt_name,verbose=False)\n",
    "\n",
    "template=\"suggest me menu 6 food items for {res_name} display with only item name remove stars \"\n",
    "prompt_name=PromptTemplate(llm=llm,template=template,input_variables=['res_name'])\n",
    "items= LLMChain(llm=llm,prompt=prompt_name)\n",
    "items.run('The American Dream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eddaa1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **The American Dream Burger**\n",
      "2. **Mac and Cheese**\n",
      "3. **Fried Chicken**\n",
      "4. **Fries**\n",
      "5. **Milkshake**\n",
      "6. **Apple Pie**\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "chains= SimpleSequentialChain(chains=[res_name,items])\n",
    "res=chains.run('american')\n",
    "for response in res.split('\\n'):\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "748f9f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate,LLMChain\n",
    "\n",
    "template=\"i have restaurant of {cusine} food. suggest me 1 name.\"\n",
    "prompt_name=PromptTemplate(template=template,input_variables=['cusine'])\n",
    "\n",
    "chain1= LLMChain(llm=llm,prompt=prompt_name,output_key='res_name')\n",
    "\n",
    "prompt=\"suggest me menu 6 food items for {res_name} display with only item name\"\n",
    "prompt_name=PromptTemplate(template=prompt,input_variables=['res_name'])\n",
    "chain2= LLMChain(llm=llm,prompt=prompt_name,output_key='items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fb5ea03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cusine': 'american',\n",
       " 'res_name': '**The American Dream**',\n",
       " 'items': '**The American Dream**\\n\\n* **The Original American Burger**\\n* **Mac & Cheese**\\n* **Fried Chicken**\\n* **Apple Pie**\\n* **Chocolate Chip Cookies**\\n* **Milkshake**'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "chain=SequentialChain(chains=[chain1,chain2],input_variables=['cusine'],output_variables=['res_name','items'])\n",
    "res=chain({'cusine':'american'})\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13a6ae1",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a7b979e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to know where the tajmahal is located\n",
      "Action: Wikipedia\n",
      "Action Input: taj mahal\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Taj Mahal\n",
      "Summary: The Taj Mahal (; lit. 'Crown of the Palace') is an ivory-white marble mausoleum on the right bank of the river Yamuna in Agra, Uttar Pradesh, India. It was commissioned in 1631 by the fifth Mughal emperor, Shah Jahan (r. 1628–1658) to house the tomb of his beloved wife, Mumtaz Mahal; it also houses the tomb of Shah Jahan himself. The tomb is the centrepiece of a 17-hectare (42-acre) complex, which includes a mosque and a guest house, and is set in formal gardens bounded on three sides by a crenellated wall.\n",
      "Construction of the mausoleum was completed in 1648, but work continued on other phases of the project for another five years. The first ceremony held at the mausoleum was an observance by Shah Jahan, on 6 February 1643, of the 12th anniversary of the death of Mumtaz Mahal. The Taj Mahal complex is believed to have been completed in its entirety in 1653 at a cost estimated at the time to be around ₹5 million, which in 2023 would be approximately ₹35 billion (US$77.8 million).\n",
      "The building complex incorporates the design traditions of Indo-Islamic and Mughal architecture. It employs symmetrical constructions with the usage of various shapes and symbols. While the mausoleum is constructed of white marble inlaid with semi-precious stones, red sandstone was used for other buildings in the complex similar to the Mughal era buildings of the time. The construction project employed more than 20,000 workers and artisans under the guidance of a board of architects led by Ustad Ahmad Lahori, the emperor's court architect.\n",
      "The Taj Mahal was designated as a UNESCO World Heritage Site in 1983 for being \"the jewel of Islamic art in India and one of the universally admired masterpieces of the world's heritage\". It is regarded as one of the best examples of Mughal architecture and a symbol of Indian history. The Taj Mahal is a major tourist attraction and attracts more than five million visitors a year. In 2007, it was declared a winner of the New 7 Wonders of the World initiative.\n",
      "\n",
      "Page: Taj Mahal (musician)\n",
      "Summary: Henry St. Claire Fredericks Jr. (born May 17, 1942), better known by his stage name Taj Mahal, is an American blues musician. He plays the guitar, piano, banjo, harmonica, and many other instruments, often incorporating elements of world music into his work. Mahal has done much to reshape the definition and scope of blues music over the course of his more than 50-year career by fusing it with nontraditional forms, including sounds from the Caribbean, Africa, India, Hawaii, and the South Pacific.\n",
      "\n",
      "\n",
      "\n",
      "Page: Taj Mahal Palace Hotel\n",
      "Summary: The Taj Mahal Palace is a heritage, five-star, luxury hotel in the Colaba area of Mumbai, Maharashtra, India, situated next to the Gateway of India. Built in the Indo-Saracenic style, it opened in 1903 as the Taj Mahal Hotel and has historically often been known simply as \"The Taj\". The hotel is named after the Taj Mahal, which is located in the city of Agra approximately 1,050 kilometres (650 mi) from Mumbai. It has been considered one of the finest hotels in the East since the time of the British Raj. The hotel was one of the main targets in the 2008 Mumbai attacks.\n",
      "Part of the Taj Hotels Resorts and Palaces, the hotel has 560 rooms and 44 suites and is considered the flagship property of the group; it employs 1,600 staff. The hotel is made up of two different structures: the Taj Mahal Palace and the Tower, which are historically and architecturally distinct from each other (the Taj Mahal Palace was built in 1903; the Tower was opened in 1972).\n",
      "The hotel has received many notable guests, from presidents to captains of industry and movie stars.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe Taj Mahal is located in Agra, Uttar Pradesh, India\n",
      "Action: Calculator\n",
      "Action Input: 2024 - 1648\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 376\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The Taj Mahal was completed in 1648 and is located in Agra, Uttar Pradesh, India. It has been 376 years since the Taj Mahal was completed.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Taj Mahal was completed in 1648 and is located in Agra, Uttar Pradesh, India. It has been 376 years since the Taj Mahal was completed.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain.agents import AgentType,initialize_agent,load_tools\n",
    "from langchain.llms import GooglePalm\n",
    "\n",
    "tools=load_tools([\"wikipedia\",\"llm-math\"],llm=llm)\n",
    "agent = initialize_agent(tools,llm,agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,verbose=True)\n",
    "agent.run(\"where was tajmahal located?.how many years done upto 2024?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7c1af27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getenv('SERP_API_KEY')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ['SERPAPI_API_KEY'] = 'a7f675d8031e0e626155303704b73ea4c169cdc97cfe2266e98086fe28f27ff8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "16cb0ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to search for the gross income tax of average salary getting employee in india\n",
      "Action: Search\n",
      "Action Input: average salary getting employee in india\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAverage Salary of a Person in India As per the Glassdoor data, the monthly salary provided to an Indian citizen ranges between Rs. 8,000 to Rs. 1,43,000 in 2024, with the annual average base pay amounting to around Rs. 9,45,489.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to calculate the gross income tax of average salary getting employee in india\n",
      "Action: Calculator\n",
      "Action Input: 945489*0.30\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 283646.7\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: 283646.7\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'283646.7'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agents\n",
    "\n",
    "from langchain.agents import AgentType,initialize_agent,load_tools\n",
    "from langchain.llms import GooglePalm\n",
    "\n",
    "tools=load_tools(['serpapi',\"llm-math\"],llm=llm)\n",
    "agent = initialize_agent(tools,llm,agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,verbose=True)\n",
    "agent.run(\"what is the gross income tax of average salary getting employee in india?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ecbbf56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mgive me 2 colors of rainbow\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "**Red** and **violet**\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "template= \"give me {num} colors of rainbow\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"num\"])\n",
    "chains = LLMChain(prompt=prompt, llm=llm, verbose=True)\n",
    "\n",
    "name=chains.run(2)\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ed92b7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Config',\n",
       " '__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__class_vars__',\n",
       " '__config__',\n",
       " '__custom_root_type__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__exclude_fields__',\n",
       " '__fields__',\n",
       " '__fields_set__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_validators__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__include_fields__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__json_encoder__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__or__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__post_root_validators__',\n",
       " '__pre_root_validators__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__ror__',\n",
       " '__schema_cache__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__signature__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__try_update_forward_refs__',\n",
       " '__validators__',\n",
       " '__weakref__',\n",
       " '_abatch_with_config',\n",
       " '_abc_impl',\n",
       " '_acall',\n",
       " '_acall_with_config',\n",
       " '_atransform_stream_with_config',\n",
       " '_batch_with_config',\n",
       " '_calculate_keys',\n",
       " '_call',\n",
       " '_call_with_config',\n",
       " '_chain_type',\n",
       " '_copy_and_set_values',\n",
       " '_decompose_class',\n",
       " '_enforce_dict_if_root',\n",
       " '_get_value',\n",
       " '_init_private_attributes',\n",
       " '_is_protocol',\n",
       " '_iter',\n",
       " '_lc_kwargs',\n",
       " '_run_output_key',\n",
       " '_transform_stream_with_config',\n",
       " '_validate_inputs',\n",
       " '_validate_outputs',\n",
       " 'abatch',\n",
       " 'acall',\n",
       " 'ainvoke',\n",
       " 'apply',\n",
       " 'arun',\n",
       " 'astream',\n",
       " 'atransform',\n",
       " 'batch',\n",
       " 'bind',\n",
       " 'callback_manager',\n",
       " 'callbacks',\n",
       " 'chains',\n",
       " 'construct',\n",
       " 'copy',\n",
       " 'dict',\n",
       " 'from_orm',\n",
       " 'input_keys',\n",
       " 'input_variables',\n",
       " 'invoke',\n",
       " 'json',\n",
       " 'lc_attributes',\n",
       " 'lc_namespace',\n",
       " 'lc_secrets',\n",
       " 'lc_serializable',\n",
       " 'map',\n",
       " 'memory',\n",
       " 'metadata',\n",
       " 'output_keys',\n",
       " 'output_variables',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'prep_inputs',\n",
       " 'prep_outputs',\n",
       " 'raise_callback_manager_deprecation',\n",
       " 'return_all',\n",
       " 'run',\n",
       " 'save',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'set_verbose',\n",
       " 'stream',\n",
       " 'tags',\n",
       " 'to_json',\n",
       " 'to_json_not_implemented',\n",
       " 'transform',\n",
       " 'update_forward_refs',\n",
       " 'validate',\n",
       " 'validate_chains',\n",
       " 'verbose',\n",
       " 'with_config',\n",
       " 'with_fallbacks',\n",
       " 'with_retry']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "91e2af29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chain.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2f72c903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory=ConversationBufferMemory()\n",
    "\n",
    "template= \"suggest me {num} best names for a boy in 2024\"\n",
    "prompt=PromptTemplate(template=template,verbose=True,input_variables=['num'])\n",
    "chain= LLMChain(llm=llm,prompt=prompt,memory=memory)\n",
    "name=chain.run(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1793835b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. **Asher**\\n2. **Elijah**'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8d07b851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_memory=ChatMessageHistory(messages=[HumanMessage(content='2', additional_kwargs={}, example=False), AIMessage(content='1. **Asher**\\n2. **Elijah**', additional_kwargs={}, example=False)]) output_key=None input_key=None return_messages=False human_prefix='Human' ai_prefix='AI' memory_key='history'\n"
     ]
    }
   ],
   "source": [
    "print(chain.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2d59634a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: 2\n",
      "AI: 1. **Asher**\n",
      "2. **Elijah**\n"
     ]
    }
   ],
   "source": [
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7a6ebd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "convo=ConversationChain(llm=llm)\n",
    "print(convo.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7661d64e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mahatma Gandhi'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"who is father of nation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1ef7a6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Amaravati'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"what is capital of ap?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1be4a2da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' YS Jagan Mohan Reddy'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\" who is the cm of ap?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2f1af0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: who is father of nation?\n",
      "AI: Mahatma Gandhi\n",
      "Human: what is capital of ap?\n",
      "AI: Amaravati\n",
      "Human:  who is the cm of ap?\n",
      "AI:  YS Jagan Mohan Reddy\n"
     ]
    }
   ],
   "source": [
    "print(convo.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "88a57b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mahendra Singh Dhoni (born 7 July 1981) is an Indian cricketer who captained the Indian national team in limited-overs formats from 2007 to 2017 and in Test cricket from 2008 to 2014.'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory=ConversationBufferWindowMemory(k=1)\n",
    "\n",
    "convo=ConversationChain(llm=llm,memory=memory)\n",
    "convo.run(\" who is msdhoni? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "98611057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'39'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\" what is his age?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "875829cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New York City'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\" what is his birthpalce ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2a5d2a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human:  what is his birthpalce ?\n",
      "AI: New York City\n"
     ]
    }
   ],
   "source": [
    "print(convo.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4beb9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
